{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.2: Work distribution\n",
    "\n",
    "Once we have created the workers (i.e. teams and threads as seen in [Lab2.1](lab2.1.ipynb)), the question is how do we assign work to each of the threads. We have already seen some examples of work assignment. In this lab we will explore how can we further control the work that gets assigned to the workers.\n",
    "\n",
    "## Table of content\n",
    "\n",
    "1. Parallel regions\n",
    "2. Controlling threadas\n",
    "    1. Masked directive\n",
    "    2. Single directive\n",
    "    3. Critical directive\n",
    "    4. Sections directive\n",
    "    5. Barriers\n",
    "3. Worksharing loops\n",
    "    2. For directive\n",
    "4. Loop scheduling\n",
    "    1. Static scheduling\n",
    "    2. Dynamic scheduling\n",
    "    3. Guided scheduling\n",
    "    4. Runtime scheduling\n",
    "    5. Auto scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel regions\n",
    "\n",
    "As seen previously, the `parallel`` directive allows to generate threads. However, parallel directives have an associated region of code that corresponds to the work that each thread is to perform. This is, the enclosed region of code that is associated with the parallel region. \n",
    "\n",
    "```C\n",
    "int i = 0;\n",
    "#pragma omp parallel\n",
    "{\n",
    "    // This is the work associated to each thread\n",
    "    #pragma omp atomic \n",
    "    i = i + 1;\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build\n",
    "!srun -N 1 -c 8 clang -fopenmp C/simple_parallel.c -o C/simple_parallel.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 4\n"
     ]
    }
   ],
   "source": [
    "#execute\n",
    "!srun -N 1 -c 8 C/./simple_parallel.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To play with this code go to [simple_parallel.c](C/simple_parallel.c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since parallel regions use the fork-joint model, each thread is assigned the same functionality. Therefore, the first work assigment we do in parallel regions is equivalent to \"every thread executes the same region\".\n",
    "\n",
    "In many cases we want to be able to change this behavior and provide more control over work assignment. The rest of this section will focus on this.\n",
    "\n",
    "It is possible to use the thread ID to identify the different threads and assign different functionalities. Take for example the following code (`NOTE: This code is not recommended and is somewhat a bad practice`:)\n",
    "\n",
    "```C\n",
    "#pragma omp parallel num_threads(5)\n",
    "{\n",
    "    if(omp_get_thread_num() == 0) {\n",
    "        printf(\"Hi, I am thread 0\\n\");\n",
    "    }\n",
    "    if(omp_get_thread_num() == 4) {\n",
    "        printf(\"And I am thread 4\\n\");\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build\n",
    "!srun -N 1 -c 8 clang -fopenmp C/not_recommended_parallel_if.c -o C/not_recommended_parallel_if.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I am thread 0\n",
      "And I am thread 4\n"
     ]
    }
   ],
   "source": [
    "#execute\n",
    "!srun -N 1 -c 8 C/./not_recommended_parallel_if.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there is a better way to control work assignment for threads as we will se below.\n",
    "\n",
    "To play with the code above go to [not_recommended_parallel_if.c](C/not_recommended_parallel_if.c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked directive\n",
    "The `masked` directive (formerly known as `master` but now featuring additional functionality) allows to select a single thread for execution of a region. This directive automatically performs the thread ID check for us, allowing to write segment of code to be associated with a single thread. \n",
    "\n",
    "The `filter()` clause can be used to control the threadID. By default the threadID chosen by the masked directive is threadID == 0. The same code above can be re-written as:\n",
    "\n",
    "```C\n",
    "#pragma omp parallel num_threads(5)\n",
    "{\n",
    "    #pragma omp masked\n",
    "        printf(\"Hi, I am thread 0\\n\");\n",
    "    #pragma omp masked filter(4)\n",
    "        printf(\"And I am thread 4\\n\");\n",
    "}\n",
    "```\n",
    "\n",
    "Notice that the first `masked` does not uses the `filter` directive, resulting in the equivalent `if(omp_get_thread_num() == 0)`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build\n",
    "!srun -N 1 -c 8 clang -fopenmp C/parallel_masked.c -o C/parallel_masked.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I am thread 0\n",
      "And I am thread 4\n"
     ]
    }
   ],
   "source": [
    "#execute\n",
    "!srun -N 1 -c 8 C/./parallel_masked.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To play with this code go to [parallel_masked.c](C/parallel_masked.c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single directive\n",
    "The mask directive allows us to specify a single thread, however, what if we do not really care which thread executes the region, as long as it is only one. The `single` directive has a similar effect to `masked`, but instead of selecting the thread, any thread may be in charge of executing the enclosed region. Take for example the following code:\n",
    "\n",
    "```C\n",
    "#pragma omp parallel num_threads(10)\n",
    "{\n",
    "    #pragma omp single\n",
    "        printf(\"1, I am thread %d\\n\", omp_get_thread_num());\n",
    "    #pragma omp single\n",
    "        printf(\"2, I am thread %d\\n\", omp_get_thread_num());\n",
    "    #pragma omp single\n",
    "        printf(\"3, I am thread %d\\n\", omp_get_thread_num());\n",
    "} \n",
    "```\n",
    "\n",
    "In this example, the three regins will be executed in parallel. Each region will be executed only once. And the thread that executes the region can be any among the 10 threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build\n",
    "!srun -N 1 -c 8 clang -fopenmp C/parallel_single.c -o C/parallel_single.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, I am thread 3\n",
      "2, I am thread 0\n",
      "3, I am thread 4\n"
     ]
    }
   ],
   "source": [
    "#execute\n",
    "!srun -N 1 -c 8 C/./parallel_single.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the example code multiple times, and notice if the thread that executes the region changes every time. \n",
    "\n",
    "To play with this code open the fine [parallel_single.c](C/parallel_single.c)\n",
    "\n",
    "Another difference with the masked directive is that single has an implicit barrier at the end of the region. Meaning, all threads must synchronize before continuing the execution of the region after the single.\n",
    "\n",
    "```C\n",
    "int i = 0;\n",
    "#pragma omp parallel\n",
    "{\n",
    "    #pragma omp single\n",
    "        i = omp_get_num_threads();\n",
    "    #pragma omp atomic\n",
    "        i = i - 1;\n",
    "}\n",
    "printf(\"i = %d\\n\", i);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build\n",
    "!srun -N 1 -c 8 clang -fopenmp C/single_barrier.c -o C/single_barrier.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0\n"
     ]
    }
   ],
   "source": [
    "#execute\n",
    "!srun -N 1 -c 8 C/./single_barrier.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above works well because before performing the atomic operation, all threads must wait for the single operaiton to finish executing.\n",
    "\n",
    "You can play with this code in [single_barrier.c](C/single_barrier.c)\n",
    "\n",
    "It is possible to remove the implicit barrier introduced by the `single` directive by using the `nowait` clause. This will allow for the other threads to execute in parallel with the single region, while still allowing only a single thread to execute the single region.\n",
    "\n",
    "### Exercise 1 - Discussion question\n",
    "Can you explain why the following code not always returns 0?\n",
    "\n",
    "```C\n",
    "int i = 0;\n",
    "#pragma omp parallel\n",
    "{\n",
    "    #pragma omp single nowait\n",
    "        i = omp_get_num_threads();\n",
    "    #pragma omp atomic\n",
    "        i = i - 1;\n",
    "}\n",
    "printf(\"i = %d\\n\", i);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build\n",
    "!srun -N 1 -c 8 clang -fopenmp Exercises/single_no_barrier.c -o Exercises/single_no_barrier.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0\n",
      "i = 1\n",
      "i = 0\n",
      "i = 0\n",
      "i = 0\n",
      "i = 18\n",
      "i = 0\n",
      "i = 2\n",
      "i = 3\n",
      "i = 0\n"
     ]
    }
   ],
   "source": [
    "#execute 10 times\n",
    "!for i in `seq 10`; do srun -N 1 -c 8 Exercises/./single_no_barrier.exe; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play with this code in [Exercises/single_no_barrier.c](Exercises/single_no_barrier.c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critical directive\n",
    "Neither the `single` or the `masked` directives are mutually exclussive between threads. This means, that while a single or masked region executes, it is possible for other threads to perform operations. \n",
    "\n",
    "Critical sections are sections that have a guaranteed mutual exclusion. Mutual exclusion is an important property of parallel programming since it protects access to sections of the code that are not thread safe. Unlike atomic operations that can only affect a single memory region, mutual exclusion allows to guard a section of code, rather than a single memory location. \n",
    "\n",
    "Let's take the following **broken** code as an example.\n",
    "\n",
    "```C\n",
    "int i, j[10] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n",
    "\n",
    "#pragma omp parallel\n",
    "{\n",
    "    i = omp_get_thread_num();\n",
    "    j[i] = 99;\n",
    "}\n",
    "\n",
    "for (i = 0; i < 10; i++)\n",
    "    printf(\"j[%d] = %d\\n\", i, j[i]);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build\n",
    "!srun -N 1 -c 8 clang -fopenmp C/broken_code_no_critical.c -o C/broken_code_no_critical.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j[0] = 99\n",
      "j[1] = 99\n",
      "j[2] = 99\n",
      "j[3] = 99\n",
      "j[4] = 99\n",
      "j[5] = 99\n",
      "j[6] = 0\n",
      "j[7] = 0\n",
      "j[8] = 0\n",
      "j[9] = 0\n"
     ]
    }
   ],
   "source": [
    "#execute\n",
    "!srun -N 1 -c 8 C/./broken_code_no_critical.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code clearly has a race condition on i, resulting in an incorrect memory access on j.\n",
    "\n",
    "Can we fix this by using atomic operations? Give it a try in [C/broken_code_no_critica.c](C/broken_code_no_critica.c)\n",
    "\n",
    "The answer is no. The reason is that there is a set of operations that spawn beyond a single memory access that need to occur exclussively from one another. Imagine the following execution order of the code above:\n",
    "\n",
    "```\n",
    "T1: i = 1;\n",
    "T2: i = 2;\n",
    "T1: j[i] = 99;\n",
    "T2: j[i] = 99;\n",
    "\n",
    "```\n",
    "\n",
    "In this case, T2 will have the right result, but the fact that i was modified before T1 was able to use the result of `omp_get_thread_num()` implies that `j[1] = 99` cannot occur. \n",
    "\n",
    "In general one must be careful about operations that need to happen exclusively. Whenever the oepration goes beyond a simple atomic memory access, it is necessary to use critical regions.\n",
    "\n",
    "The `critical` directive allows us to do exactly this. We use `critical` to select a large number of operations that are known not to be thread safe (e.g., using a function that is not thread safe), to guarantee that only a single thread executes such function.\n",
    "\n",
    "Critical does not reduce the number of executions in the region, but just guarantees that each execution is independent from each other. Take for example the following code that uses an array to store the order in which threads have executed the critical region.\n",
    "\n",
    "```C\n",
    "int i = 0, j[10] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n",
    "\n",
    "#pragma omp parallel\n",
    "{\n",
    "    #pragma omp critical\n",
    "    {\n",
    "        i++;\n",
    "        j[i] = omp_get_thread_num();\n",
    "        #pragma omp flush\n",
    "    }\n",
    "}\n",
    "\n",
    "for (i = 0; i < 10; i++)\n",
    "    printf(\"j[%d] = %d\\n\", i, j[i]);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build\n",
    "!srun -N 1 -c 8 clang -fopenmp C/critical_array_order.c -o C/critical_array_order.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread 0 was the 1th to execute the critical section.\n",
      "Thread 4 was the 2th to execute the critical section.\n",
      "Thread 3 was the 3th to execute the critical section.\n",
      "Thread 0 was the 4th to execute the critical section.\n",
      "Thread 2 was the 5th to execute the critical section.\n",
      "Thread 1 was the 6th to execute the critical section.\n",
      "Thread 5 was the 7th to execute the critical section.\n",
      "Thread 0 was the 8th to execute the critical section.\n",
      "Thread 0 was the 9th to execute the critical section.\n",
      "Thread 0 was the 10th to execute the critical section.\n"
     ]
    }
   ],
   "source": [
    "#execute\n",
    "!srun -N 1 -c 8 C/./critical_array_order.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play with this code in [C/critical_array_order.c](C/critical_array_order.c).\n",
    "\n",
    "```\n",
    "NOTE: The #pragma omp flush is necessary because i may be stored in a register in a given thread. We must guarantee that the register is flushed into memory such that other threads can see the updated value of i. This is, in order to \"flush\" the effects of the execution of this region to memory, such that other threads can see the new value of i, we must guaranteed that the data has been written back to memory. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 2: Critical regions for producer/consumer queue\n",
    "\n",
    "The following program has a problem. The operation between producer and consumer is not thread safe, and has not been guarded properly. For this reason, it is likely this program never finishes, or gives unexpected results (it could even break during runtime). \n",
    "\n",
    "Can you modify the file [Exercises/critical_region_queue.cpp](Exercises/critical_region_queue.cpp) to fix this issue by using critical regions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build\n",
    "!srun -N 1 -c 8 clang++ -fopenmp Exercises/critical_region_queue.cpp -o Exercises/critical_region_queue.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "double free or corruption (out)\n"
     ]
    }
   ],
   "source": [
    "#execute\n",
    "!srun -N 1 -c 8 Exercises/./critical_region_queue.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A possible solution to this excercise can be found in [Solutions/critical_region_queue.cpp](Solutions/critical_region_queue.cpp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build\n",
    "!srun -N 1 -c 8 clang++ -fopenmp Solutions/critical_region_queue.cpp -o Solutions/critical_region_queue.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consumer took 83 from the queue.\n",
      "Consumer took 86 from the queue.\n",
      "Consumer took 77 from the queue.\n",
      "Consumer took 15 from the queue.\n",
      "Consumer took 93 from the queue.\n",
      "Consumer took 35 from the queue.\n",
      "Consumer took 86 from the queue.\n",
      "Consumer took 92 from the queue.\n",
      "Consumer took 49 from the queue.\n",
      "Consumer took 21 from the queue.\n",
      "Consumer took 62 from the queue.\n",
      "Consumer took 27 from the queue.\n",
      "Consumer took 90 from the queue.\n",
      "Consumer took 59 from the queue.\n",
      "Consumer took 63 from the queue.\n",
      "Consumer took 26 from the queue.\n",
      "Consumer took 40 from the queue.\n",
      "Consumer took 26 from the queue.\n",
      "Consumer took 72 from the queue.\n",
      "Consumer took 36 from the queue.\n",
      "Consumer took 11 from the queue.\n",
      "Consumer took 68 from the queue.\n",
      "Consumer took 67 from the queue.\n",
      "Consumer took 29 from the queue.\n",
      "Consumer took 82 from the queue.\n",
      "Consumer took 30 from the queue.\n",
      "Consumer took 62 from the queue.\n",
      "Consumer took 23 from the queue.\n",
      "Consumer took 67 from the queue.\n",
      "Consumer took 35 from the queue.\n",
      "Consumer took 29 from the queue.\n",
      "Consumer took 2 from the queue.\n",
      "Consumer took 22 from the queue.\n",
      "Consumer took 58 from the queue.\n",
      "Consumer took 69 from the queue.\n",
      "Consumer took 67 from the queue.\n",
      "Consumer took 93 from the queue.\n",
      "Consumer took 56 from the queue.\n",
      "Consumer took 11 from the queue.\n",
      "Consumer took 42 from the queue.\n",
      "Consumer took 29 from the queue.\n",
      "Consumer took 73 from the queue.\n",
      "Consumer took 21 from the queue.\n",
      "Consumer took 19 from the queue.\n",
      "Consumer took 84 from the queue.\n",
      "Consumer took 37 from the queue.\n",
      "Consumer took 98 from the queue.\n",
      "Consumer took 24 from the queue.\n",
      "Consumer took 15 from the queue.\n",
      "Consumer took 70 from the queue.\n",
      "Consumer took 13 from the queue.\n",
      "Consumer took 26 from the queue.\n",
      "Consumer took 91 from the queue.\n",
      "Consumer took 80 from the queue.\n",
      "Consumer took 56 from the queue.\n",
      "Consumer took 73 from the queue.\n",
      "Consumer took 62 from the queue.\n",
      "Consumer took 70 from the queue.\n",
      "Consumer took 96 from the queue.\n",
      "Consumer took 81 from the queue.\n",
      "Consumer took 5 from the queue.\n",
      "Consumer took 25 from the queue.\n",
      "Consumer took 84 from the queue.\n",
      "Consumer took 27 from the queue.\n",
      "Consumer took 36 from the queue.\n",
      "Consumer took 5 from the queue.\n",
      "Consumer took 46 from the queue.\n",
      "Consumer took 29 from the queue.\n",
      "Consumer took 13 from the queue.\n",
      "Consumer took 57 from the queue.\n",
      "Consumer took 24 from the queue.\n",
      "Consumer took 95 from the queue.\n",
      "Consumer took 82 from the queue.\n",
      "Consumer took 45 from the queue.\n",
      "Consumer took 14 from the queue.\n",
      "Consumer took 67 from the queue.\n",
      "Consumer took 34 from the queue.\n",
      "Consumer took 64 from the queue.\n",
      "Consumer took 43 from the queue.\n",
      "Consumer took 50 from the queue.\n",
      "Consumer took 87 from the queue.\n",
      "Consumer took 8 from the queue.\n",
      "Consumer took 76 from the queue.\n",
      "Consumer took 78 from the queue.\n",
      "Consumer took 88 from the queue.\n",
      "Consumer took 84 from the queue.\n",
      "Consumer took 3 from the queue.\n",
      "Consumer took 51 from the queue.\n",
      "Consumer took 54 from the queue.\n",
      "Consumer took 99 from the queue.\n",
      "Consumer took 32 from the queue.\n",
      "Consumer took 60 from the queue.\n",
      "Consumer took 76 from the queue.\n",
      "Consumer took 68 from the queue.\n",
      "Consumer took 39 from the queue.\n",
      "Consumer took 12 from the queue.\n",
      "Consumer took 26 from the queue.\n",
      "Consumer took 86 from the queue.\n",
      "Consumer took 94 from the queue.\n",
      "Consumer took 39 from the queue.\n"
     ]
    }
   ],
   "source": [
    "#execute\n",
    "!srun -N 1 -c 8 Solutions/./critical_region_queue.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sections directive\n",
    "\n",
    "Another possibility for work assigment to threads is the use of `sections` directive. This directive allows to specify different worksharing sections that are to be executed by different threads. Each section will be assigned a different thread during runtime, resulting in a collection of threads each with independent behavior. \n",
    "\n",
    "Assume the following struct exists:\n",
    "\n",
    "```C\n",
    "struct someStruct\n",
    "{\n",
    "    int a;\n",
    "    int b;\n",
    "    int c;\n",
    "};\n",
    "```\n",
    "\n",
    "Take for example the following code:\n",
    "\n",
    "```C\n",
    "    struct someStruct s[10];\n",
    "    #pragma omp parallel num_threads(5)\n",
    "    {\n",
    "        #pragma omp sections\n",
    "        {\n",
    "            #pragma omp section\n",
    "            for (int i = 0; i < 10; i++)\n",
    "                s[i].a = i;\n",
    "            #pragma omp section\n",
    "            for (int i = 0; i < 10; i++)\n",
    "                s[i].b = i;\n",
    "            #pragma omp section\n",
    "            for (int i = 0; i < 10; i++)\n",
    "                s[i].c = i;\n",
    "        }\n",
    "    }\n",
    "\n",
    "```\n",
    "\n",
    "Notice how each thread is executing on an independent memory region of the array of structs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build\n",
    "!srun -N 1 -c 8 clang -fopenmp C/sections.c -o C/sections.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s[0].a = 0\n",
      "s[0].b = 0\n",
      "s[0].c = 0\n",
      "s[1].a = 1\n",
      "s[1].b = 1\n",
      "s[1].c = 1\n",
      "s[2].a = 2\n",
      "s[2].b = 2\n",
      "s[2].c = 2\n",
      "s[3].a = 3\n",
      "s[3].b = 3\n",
      "s[3].c = 3\n",
      "s[4].a = 4\n",
      "s[4].b = 4\n",
      "s[4].c = 4\n",
      "s[5].a = 5\n",
      "s[5].b = 5\n",
      "s[5].c = 5\n",
      "s[6].a = 6\n",
      "s[6].b = 6\n",
      "s[6].c = 6\n",
      "s[7].a = 7\n",
      "s[7].b = 7\n",
      "s[7].c = 7\n",
      "s[8].a = 8\n",
      "s[8].b = 8\n",
      "s[8].c = 8\n",
      "s[9].a = 9\n",
      "s[9].b = 9\n",
      "s[9].c = 9\n"
     ]
    }
   ],
   "source": [
    "#execute\n",
    "!srun -N 1 -c 8 C/./sections.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barriers\n",
    "\n",
    "Another important mechanism for threads is the need to synchronize threads. More exactly. to guarantee that all the threads have reached a specific part of the program before it can continue the execution. \n",
    "\n",
    "Barriers are quite useful depending on the application. Barriers are useful to guarantee that a result has already been commited before continuing with another part of the parallel program. \n",
    "\n",
    "Take the following example\n",
    "\n",
    "```C\n",
    "int arr[10];\n",
    "#pragma omp parallel num_threads(10)\n",
    "{\n",
    "    arr[omp_get_thread_num()] = omp_get_thread_num();\n",
    "    #pragma omp barrier\n",
    "    #pragma omp single\n",
    "    {\n",
    "        for (int i = 0; i < 10; i++)\n",
    "        {\n",
    "            printf(\"arr[%d] = %d\\n\", i, arr[i]);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The barrier guarantees that all the threads have written their threadId into `arr`. Allowing the print to have the expected result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build\n",
    "!srun -N 1 -c 8 clang -fopenmp C/barriers.c -o C/barriers.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr[0] = 0\n",
      "arr[1] = 1\n",
      "arr[2] = 2\n",
      "arr[3] = 3\n",
      "arr[4] = 4\n",
      "arr[5] = 5\n",
      "arr[6] = 6\n",
      "arr[7] = 7\n",
      "arr[8] = 8\n",
      "arr[9] = 9\n"
     ]
    }
   ],
   "source": [
    "#execute\n",
    "!srun -N 1 -c 8 C/./barriers.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there are many use cases, a common use is that we can reuse the parallelism that has already been generated. In general, thread creation can be a costly operation in some instances. By reusing the parallel region, we reduce the overhead of the runtime. Let us create a simple benchmark that compares the two schemes\n",
    "\n",
    "```C\n",
    "\n",
    "auto start = std::chrono::high_resolution_clock::now();\n",
    "for (int i = 0; i < 1000; i++)\n",
    "    for (int j = 0; j < 1000; j++)\n",
    "        #pragma omp parallel\n",
    "            f();\n",
    "auto end = std::chrono::high_resolution_clock::now();\n",
    "std::chrono::duration<double> diff = end - start;\n",
    "printf(\"Time independent parallel regions: %f\\n\", diff.count());\n",
    "\n",
    "start = std::chrono::high_resolution_clock::now();\n",
    "for (int i = 0; i < 1000; i++)\n",
    "    #pragma omp parallel\n",
    "        for (int j = 0; j < 1000; j++)\n",
    "        {\n",
    "            f();\n",
    "            #pragma omp barrier\n",
    "        }\n",
    "end = std::chrono::high_resolution_clock::now();\n",
    "diff = end - start;\n",
    "printf(\"Time single parallel region with barriers: %f\\n\", diff.count());\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build\n",
    "!srun -N 1 -c 8 clang++ -fopenmp C/parallel_time_creation.cpp -o C/parallel_time_creation.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time independent parallel regions: 0.643148\n",
      "Time single parallel region with barriers: 0.346148\n"
     ]
    }
   ],
   "source": [
    "#execute\n",
    "!srun -N 1 -c 8 C/./parallel_time_creation.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worksharing loops\n",
    "\n",
    "Loops are really common in programs and are important in the context of worksharing, because it is sometimes possible to distribute the iteration space of a loop across the different worker threads. In Loop worksharing each thread will get a subset of the iteration space of the loop, and it will execute only that given subset. \n",
    "\n",
    "Loops can be expressed in many ways (e.g. while, for, do-while, etc), but for loop worksharing, they must be in the canonical form. \n",
    "\n",
    "In OpenMP, a canonical loop is a loop that has the following form:\n",
    "\n",
    "```C\n",
    "for (init_expr; test_expr; incr_expr) {\n",
    "    body_stmts;\n",
    "}\n",
    "```\n",
    "\n",
    "where `init_expr` initializes the loop counter, `test_expr` tests the loop counter against a loop limit, `incr_expr` increments the loop counter, and `body_stmts` are the statements executed in each iteration of the loop.\n",
    "\n",
    "It's worth noting that not all loops can be expressed in the canonical form. For example, loops with multiple loop counters in the same loop structure, or loops with non-constant loop limits cannot be expressed in the canonical form and may require additional work to parallelize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The for directive\n",
    "\n",
    "The for directive is used to distribute a loop in a parallel region. Take a look at the following example:\n",
    "\n",
    "```C\n",
    "#pragma omp parallel\n",
    "    #pragma omp for\n",
    "    for (int i = 0; i < 10; i++)\n",
    "    {\n",
    "        printf(\"Hi from thread %d\\n, I am iteration %d\\n\", omp_get_thread_num(), i);\n",
    "    }\n",
    "```\n",
    "\n",
    "The 10 iterations will be split across the different threads created by the parallel directive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build\n",
    "!srun -N 1 -c 8 clang -fopenmp C/for_loop.c -o C/for_loop.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi from thread 2, I am iteration 4\n",
      "Hi from thread 2, I am iteration 5\n",
      "Hi from thread 0, I am iteration 0\n",
      "Hi from thread 0, I am iteration 1\n",
      "Hi from thread 4, I am iteration 8\n",
      "Hi from thread 5, I am iteration 9\n",
      "Hi from thread 1, I am iteration 2\n",
      "Hi from thread 1, I am iteration 3\n",
      "Hi from thread 3, I am iteration 6\n",
      "Hi from thread 3, I am iteration 7\n"
     ]
    }
   ],
   "source": [
    "#execute\n",
    "!srun -N 1 -c 8 C/./for_loop.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play with this code in [C/for_loop.c](C/for_loop.c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that each thread will sometimes be associated with multiple iterations. An example of an execution with 3 threads could be:\n",
    "\n",
    "`| T0 | T0 | T1 | T1 | T2 | T2 | T0 | T0 | T1 | T1 |`\n",
    "\n",
    "In the above `T#` means the thread with `ID = #`.\n",
    "\n",
    "As we will see later below, it is possible to control assigment of threads to iterations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note on variable privatization\n",
    "\n",
    "A common pitfal is to believe that data privatization happens at the iteration level, rather than at the thread level. The clauses `private`, `firstprivate` and `lastprivate` are used to modify the privatization at the thread level rather than at the iteration level. For this reason, if we consider the thread distribution above, and we use a `private` directive for a variable x, this variable will keep the value for `iterations = 0, 1, 6, 7` in thread T0. \n",
    "\n",
    "Take for example the following code:\n",
    "\n",
    "```C\n",
    "int x;\n",
    "int arr[10];\n",
    "#pragma omp parallel\n",
    "    #pragma omp for schedule(static, 3) firstprivate(x)\n",
    "    for (int i = 0; i < 10; i++)\n",
    "        arr[i] = x++;\n",
    "\n",
    "for (int i = 0; i < 10; i++)\n",
    "    printf(\"arr[%d] = %d\\n\", i, arr[i]);\n",
    "```\n",
    "\n",
    "Ignore for now the schedule clause, it will be introduced briefly, but it helps make this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build\n",
    "!srun -N 1 -c 8 clang -fopenmp C/for_private_loop.c -o C/for_private_loop.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arr[0] = 0\n",
      "arr[1] = 1\n",
      "arr[2] = 2\n",
      "arr[3] = 0\n",
      "arr[4] = 1\n",
      "arr[5] = 2\n",
      "arr[6] = 0\n",
      "arr[7] = 1\n",
      "arr[8] = 2\n",
      "arr[9] = 0\n"
     ]
    }
   ],
   "source": [
    "#execute\n",
    "!srun -N 1 -c 8 C/./for_private_loop.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the variable x is not always 0 for each iteration. Rather, each thread has a private variable x, but the value of x is kept between the iterations performed by that thread.\n",
    "\n",
    "Play with this code in [C/for_private_loop.C](C/for_private_loop.c)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data parallelism\n",
    "\n",
    "Data parallelism comes from being able to perform the same operation over large amounts of data. This is often expressed in the form of a loop, where the loop iteration variable is used to access elements of an iterable data structure (e.g., `A[i]`). Data parallelism is one of the most common parallelization patterns there exists (also heavily exploited by GPUs and alike devices).\n",
    "\n",
    "Take the following vector addition example:\n",
    "\n",
    "```C\n",
    "#pragma omp parallel for\n",
    "for (int i = 0; i < 1000; i++)\n",
    "    c[i] = a[i] + b[i];\n",
    "```\n",
    "\n",
    "In this case data parallelism is achieved by distributting the access and computation of the arrays `a`, `b`, and `c` across multiple threads. This is achieved by distributting the iteration space that is directly associated with the variable access.\n",
    "\n",
    "```NOTE: The `omp parallel for` is a combined directive that is equivalent to writing `parallel` and `for` in two lines. ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not every loop can achieve parallelism through a `parallel for` directive. The decision if a loop is or not parallelizable relies on the dependencies between iterations. This concept is first introduce in [Lab1](../Lab1/lab1.ipynb) but let us improve its definition here. \n",
    "\n",
    "In the vector addition example, arrays `a` and `b` are read, while array `c` is written. As a developer, we must track these as they could determine if a code is parallelizable. The analysis of inter-loop dependencies is performed by separating reads and writes operations in the code, and determine if a variable that is written in an iteration, is read in another iteration. \n",
    "\n",
    "Here is an example of a loop that have interloop dependencies, limitting the available parallelism.\n",
    "\n",
    "```C\n",
    "/// a[1] depends on the result of a[0]\n",
    "for (int i = 1; i < N; i++)\n",
    "    a[i] = a[i-1] + 1;\n",
    "```\n",
    "\n",
    "Here is an example of a code that cannot yet be determined if there are inter loop dependencies:\n",
    "\n",
    "```C\n",
    "/// Assuming a is a pointer\n",
    "for (int i = 0; i < N; i++)\n",
    "    f(a); // a scapes this scope, the dependency analisys relies on f()\n",
    "```\n",
    "\n",
    "In other cases where inter-loop dependencies exists, there are work arounds to achieve parallelism. This is the case of reductions:\n",
    "\n",
    "```C\n",
    "/// Reduction for commutative operations\n",
    "for (int i = 0; i < N; i++) {\n",
    "    i = i + f();\n",
    "}\n",
    "```\n",
    "\n",
    "Also, it is the case when there is some distance between dependencies in the iteration space that is grater than 1.\n",
    "\n",
    "```C\n",
    "/// a[20] depends on a[10], but not on a[11], a[12], ...\n",
    "for (int i = 10; i < N; i++)\n",
    "    a[i] = a[i-10] + 1;\n",
    "```\n",
    "\n",
    "Likewise, some interloop dependencies can be removed by using a second buffer to store the results:\n",
    "\n",
    "```C\n",
    "/// a[1] depends on the previous value in a[2]\n",
    "for (int i = 1; i < N-1; i++)\n",
    "    a[i] = a[i+1] + 1;\n",
    "\n",
    "/// But we can create a second array for the result of this loop\n",
    "int a[N], b[N];\n",
    "...\n",
    "for (int i = 1; i < N-1; i++)\n",
    "    b[i] = a[i+1] + 1; // Notice that the values of a are now read only\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop scheduling\n",
    "Loop scheduling allows us to control the assigment of threads to the iteration space. There are 5 types of scheduling policies in OpenMP:\n",
    "\n",
    "* Static\n",
    "* Dynamic\n",
    "* Guided\n",
    "* Runtime \n",
    "* Auto\n",
    "\n",
    "Here is a table that summarizes scheduling policies:\n",
    "\n",
    "| Name    | Chunk Size | Thread Scheduling |\n",
    "| ------- | ----------| ----------------- |\n",
    "| Static  | User-defined | Fixed |\n",
    "| Dynamic | User-defined | Variable |\n",
    "| Guided  | User-defined Hint | Variable |\n",
    "| Runtime | User-defined | Determined at runtime |\n",
    "| Auto    | Determined by compiler | Determined by compiler |\n",
    "\n",
    "\n",
    "Let us study each of this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static scheduling\n",
    "Static scheduling determines that the iteration space is partitioned in chunks of fixed size, and that each chunk is scheduled to a thread statically in round-robin fashion.\n",
    "\n",
    "Using code in [C/for_loop_static.c](C/for_loop_static.c).\n",
    "\n",
    "```C\n",
    "#pragma omp parallel\n",
    "    #pragma omp for schedule(static:3)\n",
    "    for (int i = 0; i < 10; i++)\n",
    "        printf(\"Hi from thread %d\\n, I am iteration %d\\n\", omp_get_thread_num(), i);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build\n",
    "!srun -N 1 -c 8 clang -fopenmp C/for_loop_static.c -o C/for_loop_static.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi from thread 0, I am iteration 0\n",
      "Hi from thread 0, I am iteration 1\n",
      "Hi from thread 0, I am iteration 2\n",
      "Hi from thread 3, I am iteration 9\n",
      "Hi from thread 2, I am iteration 6\n",
      "Hi from thread 2, I am iteration 7\n",
      "Hi from thread 2, I am iteration 8\n",
      "Hi from thread 1, I am iteration 3\n",
      "Hi from thread 1, I am iteration 4\n",
      "Hi from thread 1, I am iteration 5\n"
     ]
    }
   ],
   "source": [
    "#execute\n",
    "!srun -N 1 -c 8 C/./for_loop_static.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic scheduling\n",
    "Dynamic scheduling determines that the iteration space is partitioned in chunks of fixed size. However scheduling of each chunk to a thread is dynamically determined at runtime as the threads become free. \n",
    "\n",
    "Using code in [C/for_loop_dynamic.c](C/for_loop_dynamic.c).\n",
    "\n",
    "```C\n",
    "#pragma omp parallel\n",
    "    #pragma omp for schedule(dynamic: 3)\n",
    "    for (int i = 0; i < 10; i++)\n",
    "        printf(\"Hi from thread %d\\n, I am iteration %d\\n\", omp_get_thread_num(), i);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build\n",
    "!srun -N 1 -c 8 clang -fopenmp C/for_loop_dynamic.c -o C/for_loop_dynamic.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi from thread 5, I am iteration 3\n",
      "Hi from thread 5, I am iteration 4\n",
      "Hi from thread 5, I am iteration 5\n",
      "Hi from thread 4, I am iteration 0\n",
      "Hi from thread 4, I am iteration 1\n",
      "Hi from thread 4, I am iteration 2\n",
      "Hi from thread 0, I am iteration 9\n",
      "Hi from thread 3, I am iteration 6\n",
      "Hi from thread 3, I am iteration 7\n",
      "Hi from thread 3, I am iteration 8\n"
     ]
    }
   ],
   "source": [
    "#execute\n",
    "!srun -N 1 -c 8 C/./for_loop_dynamic.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guided scheduling\n",
    "In guided scheduling, the developer provides a hint for the chunk size, but the actual size can be adjusted dynamically at runtime. Likewise, the assignment of chunks can be dynamically scheduled at runtime.\n",
    "\n",
    "Using code in [C/for_loop_guided.c](C/for_loop_guided.c).\n",
    "\n",
    "```C\n",
    "#pragma omp parallel\n",
    "    #pragma omp for schedule(guided:3)\n",
    "    for (int i = 0; i < 20; i++)\n",
    "        printf(\"Hi from thread %d\\n, I am iteration %d\\n\", omp_get_thread_num(), i);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build\n",
    "!srun -N 1 -c 8 clang -fopenmp C/for_loop_guided.c -o C/for_loop_guided.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi from thread 0, I am iteration 3\n",
      "Hi from thread 0, I am iteration 4\n",
      "Hi from thread 0, I am iteration 5\n",
      "Hi from thread 0, I am iteration 18\n",
      "Hi from thread 1, I am iteration 12\n",
      "Hi from thread 1, I am iteration 13\n",
      "Hi from thread 5, I am iteration 15\n",
      "Hi from thread 5, I am iteration 16\n",
      "Hi from thread 5, I am iteration 17\n",
      "Hi from thread 0, I am iteration 19\n",
      "Hi from thread 1, I am iteration 14\n",
      "Hi from thread 3, I am iteration 6\n",
      "Hi from thread 3, I am iteration 7\n",
      "Hi from thread 3, I am iteration 8\n",
      "Hi from thread 4, I am iteration 0\n",
      "Hi from thread 4, I am iteration 1\n",
      "Hi from thread 4, I am iteration 2\n",
      "Hi from thread 2, I am iteration 9\n",
      "Hi from thread 2, I am iteration 10\n",
      "Hi from thread 2, I am iteration 11\n"
     ]
    }
   ],
   "source": [
    "#execute\n",
    "!srun -N 1 -c 8 C/./for_loop_guided.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto scheduling\n",
    "In Auto scheduling, the scheduling decision is leaft to the compiler. The compiler can use optimizations and heuristics to find the best scheduling option. \n",
    "\n",
    "Using code in [C/for_loop_auto.c](C/for_loop_auto.c).\n",
    "\n",
    "```C\n",
    "#pragma omp parallel\n",
    "    #pragma omp for schedule(auto)\n",
    "    for (int i = 0; i < 10; i++)\n",
    "        printf(\"Hi from thread %d\\n, I am iteration %d\\n\", omp_get_thread_num(), i);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build\n",
    "!srun -N 1 -c 8 clang -fopenmp C/for_loop_auto.c -o C/for_loop_auto.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi from thread 0, I am iteration 1\n",
      "Hi from thread 0, I am iteration 6\n",
      "Hi from thread 0, I am iteration 7\n",
      "Hi from thread 0, I am iteration 8\n",
      "Hi from thread 3, I am iteration 2\n",
      "Hi from thread 5, I am iteration 5\n",
      "Hi from thread 4, I am iteration 0\n",
      "Hi from thread 1, I am iteration 4\n",
      "Hi from thread 0, I am iteration 9\n",
      "Hi from thread 2, I am iteration 3\n"
     ]
    }
   ],
   "source": [
    "#execute\n",
    "!srun -N 1 -c 8 C/./for_loop_auto.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime scheduling\n",
    "The runtime scheduling policy allows to make changes to the scheduling at runtime, by using an environment variable\n",
    "\n",
    "Using code in [C/for_loop_runtime.c](C/for_loop_runtime.c).\n",
    "\n",
    "```C\n",
    "#pragma omp parallel\n",
    "    #pragma omp for schedule(runtime)\n",
    "    for (int i = 0; i < 10; i++)\n",
    "        printf(\"Hi from thread %d\\n, I am iteration %d\\n\", omp_get_thread_num(), i);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build\n",
    "!srun -N 1 -c 8 clang -fopenmp C/for_loop_runtime.c -o C/for_loop_runtime.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi from thread 1, I am iteration 2\n",
      "Hi from thread 1, I am iteration 3\n",
      "Hi from thread 2, I am iteration 4\n",
      "Hi from thread 2, I am iteration 5\n",
      "Hi from thread 4, I am iteration 8\n",
      "Hi from thread 0, I am iteration 0\n",
      "Hi from thread 0, I am iteration 1\n",
      "Hi from thread 5, I am iteration 9\n",
      "Hi from thread 3, I am iteration 6\n",
      "Hi from thread 3, I am iteration 7\n"
     ]
    }
   ],
   "source": [
    "#execute\n",
    "!srun -N 1 -c 8 C/./for_loop_runtime.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi from thread 1, I am iteration 5\n",
      "Hi from thread 1, I am iteration 6\n",
      "Hi from thread 1, I am iteration 7\n",
      "Hi from thread 1, I am iteration 8\n",
      "Hi from thread 1, I am iteration 9\n",
      "Hi from thread 0, I am iteration 0\n",
      "Hi from thread 0, I am iteration 1\n",
      "Hi from thread 0, I am iteration 2\n",
      "Hi from thread 0, I am iteration 3\n",
      "Hi from thread 0, I am iteration 4\n"
     ]
    }
   ],
   "source": [
    "#execute\n",
    "!OMP_SCHEDULE=\"static,5\" srun -N 1 -c 8 C/./for_loop_runtime.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi from thread 3, I am iteration 2\n",
      "Hi from thread 3, I am iteration 3\n",
      "Hi from thread 0, I am iteration 4\n",
      "Hi from thread 0, I am iteration 5\n",
      "Hi from thread 1, I am iteration 8\n",
      "Hi from thread 1, I am iteration 9\n",
      "Hi from thread 2, I am iteration 6\n",
      "Hi from thread 2, I am iteration 7\n",
      "Hi from thread 4, I am iteration 0\n",
      "Hi from thread 4, I am iteration 1\n"
     ]
    }
   ],
   "source": [
    "#execute\n",
    "!OMP_SCHEDULE=\"dynamic,2\" srun -N 1 -c 8  C/./for_loop_runtime.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise - Matrix multiplication\n",
    "\n",
    "Parallelize the following Matrix Multiplication algorithm. Modify the file [Exercises/matrix_mult.c](Exercises/matrix_mult.c).\n",
    "\n",
    "```C\n",
    "for (int i = 0; i < N; i++)\n",
    "{\n",
    "    for (int j = 0; j < N; j++)\n",
    "    {\n",
    "        *(C + i * N + j) = 0;\n",
    "        for (int k = 0; k < N; k++)\n",
    "        {\n",
    "            *(C + i * N + j) += *(A + i * N + k) * *(B + k * N + j);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build\n",
    "!srun -N 1 -c 8 clang -fopenmp Exercises/matrix_mult.c -o Exercises/matrix_mult.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285 240 195 150 105 60 15 -30 -75 -120 \n",
      "330 275 220 165 110 55 0 -55 -110 -165 \n",
      "375 310 245 180 115 50 -15 -80 -145 -210 \n",
      "420 345 270 195 120 45 -30 -105 -180 -255 \n",
      "465 380 295 210 125 40 -45 -130 -215 -300 \n",
      "510 415 320 225 130 35 -60 -155 -250 -345 \n",
      "555 450 345 240 135 30 -75 -180 -285 -390 \n",
      "600 485 370 255 140 25 -90 -205 -320 -435 \n",
      "645 520 395 270 145 20 -105 -230 -355 -480 \n",
      "690 555 420 285 150 15 -120 -255 -390 -525 \n"
     ]
    }
   ],
   "source": [
    "#execute\n",
    "!srun -N 1 -c 8 Exercises/./matrix_mult.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra points: Changing the value of N. Can you create a plot for speed up of your algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple solution in the file [Solutions/Matrix_mult.c](Solutions/matrix_mult.c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build\n",
    "!srun -N 1 -c 8 gcc -fopenmp Solutions/matrix_mult.c -o Solutions/matrix_mult.exe -O3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent in sequential: 1.925665\n",
      "Time spent in parallel: 1.915171\n"
     ]
    }
   ],
   "source": [
    "#execute\n",
    "!srun -N 1 -c 8 Solutions/./matrix_mult.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
